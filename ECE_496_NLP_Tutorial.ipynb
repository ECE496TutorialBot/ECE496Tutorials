{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ECE 496 NLP Tutorial.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNuOXyxr2cSqbJPFZvZ85KS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PenroseTiles/ECE496Tutorials/blob/master/ECE_496_NLP_Tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2HDmWlDgMsz6",
        "colab_type": "text"
      },
      "source": [
        "#SENTIMENT ANALYSIS USING LSTMs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sne1Qa0fbyaS",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://d3ansictanv2wj.cloudfront.net/SentimentAnalysis16-38b6f3cbb7bae622fe0ba114db188666.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "opXZW7EbcAG_",
        "colab_type": "code",
        "outputId": "f06b1562-f700-42c3-92ca-3f7e9d8d0c18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "%tensorflow_version 1.x\n",
        "from tensorflow.python.ops.rnn_cell_impl import LSTMStateTuple, LSTMCell, RNNCell\n",
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "\n",
        "def build_vocab(filename):\n",
        "  vocab ={'UNK':0}\n",
        "  vocab_size=1\n",
        "  for line in open(filename,'r'):\n",
        "    for word in line.split(): # this splits a sentence into a list of words\n",
        "      if word not in vocab:\n",
        "        vocab[word]=vocab_size\n",
        "        vocab_size+=1\n",
        "  return vocab"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DmXEZuBDc-Uh",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://i.pinimg.com/originals/45/5a/b7/455ab7a162e87d41cfbe167f39a03348.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDs2z83fditd",
        "colab_type": "text"
      },
      "source": [
        "#What is a tokenizer?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hphOCG8JckkE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_vocab_tokenized(filename, language='english'):\n",
        "  vocab ={'UNK':0}\n",
        "  vocab_size=len(vocab)\n",
        "  for line in open(filename,'r'):\n",
        "    for word in nltk.tokenize.word_tokenize(line, language):\n",
        "      if word not in vocab:\n",
        "        vocab[word] = vocab_size\n",
        "        vocab_size+=1\n",
        "  return vocab\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_OMOwJleUxw",
        "colab_type": "code",
        "outputId": "020514b2-0b59-443d-b028-de255f4c04e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "line = 'I am going to the market.'\n",
        "print(line.split()) #Basic tokenizer\n",
        "print(nltk.tokenize.word_tokenize('I am going to the market.')) #NLTK's tokenizer\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['I', 'am', 'going', 'to', 'the', 'market.']\n",
            "['I', 'am', 'going', 'to', 'the', 'market', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EuMfnxnyfkq8",
        "colab_type": "text"
      },
      "source": [
        "A tokenizer that just splits may not recognize the token 'market.' even though 'market' is a common word that must have occured in the dataset. When you're working with a small dataset, these things can make a difference. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eORYWVkNV016",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "english_data = 'english.txt'\n",
        "french_data = 'french.txt'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZRBIgNuDGH_",
        "colab_type": "text"
      },
      "source": [
        "#Download the datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yd8tF4Jq83UV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import requests\n",
        "url_en =\"https://github.com/nehal96/Seq2Seq-Language-Translation/raw/master/data/small_vocab_en\"\n",
        "url_fr = \"https://github.com/nehal96/Seq2Seq-Language-Translation/raw/master/data/small_vocab_fr\"\n",
        "\n",
        "remote_en = requests.get(url_en, allow_redirects=True)\n",
        "remote_fr = requests.get(url_fr, allow_redirects=True)\n",
        "with open(english_data,'wb') as f:\n",
        "  f.write(remote_en.content)\n",
        "with open(french_data,'wb') as f:\n",
        "  f.write(remote_fr.content)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nrpnYhd0DOoa",
        "colab_type": "text"
      },
      "source": [
        "#Build vocabularies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqcEFKB6B6Y9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "en_vocab = build_vocab_tokenized(english_data)\n",
        "fr_vocab = build_vocab_tokenized(french_data)\n",
        "\n",
        "en_id2word = {en_vocab[word]:word for word in en_vocab}\n",
        "fr_id2word = {fr_vocab[word]:word for word in fr_vocab}\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ctGTfbudD1cg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def words_to_ids(line, language='english'):\n",
        "  '''\n",
        "  TOKENIZES ONE SENTENCE\n",
        "  '''\n",
        "  ret = []\n",
        "  #SELECT APPROPRIATE VOCAB\n",
        "  vocab = fr_vocab if (language == 'french') else en_vocab\n",
        "  for word in nltk.tokenize.word_tokenize(line, language=language):\n",
        "    if word not in vocab:\n",
        "      ret+= [vocab['UNK']]\n",
        "    else:\n",
        "      ret += [vocab[word]]\n",
        "  return ret \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "164K8D4cEKME",
        "colab_type": "code",
        "outputId": "d28449de-fd4f-42d7-f3ef-660501307da8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "#SANITY CHECK\n",
        "\n",
        "ids = []\n",
        "for word_id in words_to_ids(\"it is snowy in april\"):\n",
        "  ids += [word_id]\n",
        "print(ids)\n",
        "\n",
        "original_sentence=[]\n",
        "for id in ids:\n",
        "  original_sentence += [en_id2word[id]]\n",
        "print(' '.join(original_sentence))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[10, 3, 11, 12, 13]\n",
            "it is snowy in april\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ipMKGPMXqeY",
        "colab_type": "code",
        "outputId": "fd4aef28-2bc3-4c32-801f-f8d2151696ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "ids_2 = []\n",
        "for word_id in words_to_ids(\"it is snowy in April\"):\n",
        "  ids_2 += [word_id]\n",
        "print(ids_2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[10, 3, 11, 12, 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzzBHlV8X0i5",
        "colab_type": "text"
      },
      "source": [
        "It is a good idea to convert all text to lowercase. Another one of those little things that can count."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9iaefFtiaQR",
        "colab_type": "code",
        "outputId": "bf0291a3-cf3e-4203-e91b-7d86174cf9a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "def pad_sentence(sentence, max_len):\n",
        "  difference = max_len - len(sentence)\n",
        "  tail = [en_vocab['UNK'] for _ in range(difference)]\n",
        "  sentence += tail\n",
        "  return sentence\n",
        "\n",
        "pad_sentence(ids,10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[10, 3, 11, 12, 13, 0, 0, 0, 0, 0]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gK7xz55yia2q",
        "colab_type": "text"
      },
      "source": [
        "Even though RNNs can take variable length inputs, it's easier to write code if you PRETEND that all inputs have the same size. In CNNs, when we pad the image with zeros around the edges, we also treat the padded pixels as part of the input image, that is not the case here. It's merely about syntactic compatibility here. We don't pass the padding to the RNN. Padding the sentences doesn't affect the output because they are omitted during computation and learning.\n",
        "\n",
        "We do this by passing the length of each sentence along with the sentence itself.\n",
        "\n",
        "\n",
        "```\n",
        "tf.nn.dynamic_rnn(rnn, inputs=word_vectors, sequence_length=sequence_lengths)\n",
        "```\n",
        "\n",
        "For the example we gave above, the sentence had 5 words, so the RNN would only be called 5 times (and not for the padded input)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8EYp5fC3f_Jh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_size = {'english' : len(en_vocab),\n",
        "              'french' : len(fr_vocab)}\n",
        "word_emb_dim = 300\n",
        "state_size = 300\n",
        "# proj_size = vocab_size['french']\n",
        "max_seq_len =100\n",
        "\n",
        "\n",
        "def create_model(lr=0.01, keep_prob=0.75):\n",
        "  inputs = tf.placeholder(tf.int32, shape=[None, max_seq_len], name='inputs')\n",
        "  outputs = tf.placeholder(tf.int64, shape=[None], name='outputs')\n",
        "  outputs_onehot = tf.to_float(tf.one_hot(outputs, 2))\n",
        "  sequence_lengths = tf.placeholder(tf.int32, shape=[None], name='seq_lens')\n",
        "  return inputs, outputs, outputs_onehot, sequence_lengths\n",
        "\n",
        "def encode(inputs, sequence_lengths):\n",
        "  # with tf.variable_scope('vs')\n",
        "  embedding_matrix = tf.get_variable('emb_matrix'+str(np.random.randint(10,1000)), shape=[vocab_size['english'], word_emb_dim],\n",
        "                                     trainable=False)\n",
        "  word_vectors = tf.nn.embedding_lookup(embedding_matrix, inputs, name=\"encoding\")\n",
        "  rnn_cell_encoder = LSTMCell(num_units = state_size)\n",
        "  initial_state = LSTMStateTuple(tf.zeros(shape=[10, state_size]), tf.zeros(shape=[10, state_size]))\n",
        "  with tf.variable_scope(\"ENCODER_RNN\"+str(np.random.randint(10,1000))):\n",
        "    outputs, state = tf.nn.dynamic_rnn(rnn_cell_encoder, inputs=word_vectors,\n",
        "                    sequence_length=sequence_lengths,\n",
        "                    dtype=tf.float32,\n",
        "                    initial_state=initial_state)\n",
        "    # print(states)\n",
        "  return state\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bb2vXHekiEyD",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQLDYLs90igr",
        "colab_type": "code",
        "outputId": "2825831f-aff5-4fd6-c1dc-13d26221e3ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 523
        }
      },
      "source": [
        "inp, labels, labels_onehot, seq_lens = create_model()\n",
        "states = encode(inp, seq_lens)\n",
        "\n",
        "w = tf.get_variable(\"W\"+str(np.random.randint(10,1000)),shape=(state_size,2))\n",
        "b = tf.get_variable(\"b\"+str(np.random.randint(10,1000)), shape=(2,))\n",
        "out = tf.nn.xw_plus_b(states[0], w, b)\n",
        "loss = tf.nn.softmax_cross_entropy_with_logits(logits=out, labels=labels_onehot)\n",
        "\n",
        "tvars = tf.trainable_variables()\n",
        "grads, _ = tf.clip_by_global_norm(tf.gradients(loss, tvars), 5.)\n",
        "optimizer = tf.train.AdamOptimizer()\n",
        "pred = tf.argmax(out, axis=1)\n",
        "print(pred)\n",
        "print(labels)\n",
        "\n",
        "accuracy = tf.reduce_mean(tf.to_float(tf.equal(pred, labels)))\n",
        "train_op = optimizer.apply_gradients(zip(grads, tvars))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-32-1395a9ffe358>:12: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "WARNING:tensorflow:From <ipython-input-32-1395a9ffe358>:21: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:From <ipython-input-32-1395a9ffe358>:27: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/rnn_cell_impl.py:958: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.add_weight` method instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/rnn_cell_impl.py:962: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/rnn.py:244: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From <ipython-input-33-408f121a9612>:7: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n",
            "Tensor(\"ArgMax:0\", shape=(10,), dtype=int64)\n",
            "Tensor(\"outputs:0\", shape=(?,), dtype=int64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0AL8Fh4xMqJ9",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}